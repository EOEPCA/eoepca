{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EO Exploitation Platform Common Architecture \u2693\ufe0e Deployment Guide","title":"Home"},{"location":"#eo-exploitation-platform-common-architecture","text":"Deployment Guide","title":"EO Exploitation Platform Common Architecture"},{"location":"about/","text":"About EOEPCA \u2693\ufe0e TBD","title":"About"},{"location":"about/#about-eoepca","text":"TBD","title":"About EOEPCA"},{"location":"deployment-guide/","text":"Deployment Guide \u2693\ufe0e The Deployment Guide describes how each building-block comprising the EOEPCA Reference Implementation is configured and deployed. A full system deployment is described, in which components are deployed with complementary configurations that facilitate their integration as a coherent system. Nevertheless, each component can be cherry-picked from this system deployment for individual re-use. The deployment is organised into the following sections: Kubernetes cluster Storage Login Service etc.","title":"Introduction"},{"location":"deployment-guide/#deployment-guide","text":"The Deployment Guide describes how each building-block comprising the EOEPCA Reference Implementation is configured and deployed. A full system deployment is described, in which components are deployed with complementary configurations that facilitate their integration as a coherent system. Nevertheless, each component can be cherry-picked from this system deployment for individual re-use. The deployment is organised into the following sections: Kubernetes cluster Storage Login Service etc.","title":"Deployment Guide"},{"location":"deployment-guide/cluster-prerequisites/","text":"Cluster Prerequisites \u2693\ufe0e The following prerequisite components are assumed to be deployed in the cluster. Nginx Ingress Controller \u2693\ufe0e # Add the helm repository helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update # Install the Nginx Ingress Controller helm chart helm upgrade -i ingress-nginx ingress-nginx/ingress-nginx --wait To target the Nginx Ingress Controller the kubernetes.io/ingress.class: nginx annotation must be applied to the Ingress resource\u2026 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx ... Cert Manager \u2693\ufe0e # Add the helm repository helm repo add jetstack https://charts.jetstack.io helm repo update # Install the Cert Manager helm chart helm upgrade -i cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs=true Letsencrypt Certificates \u2693\ufe0e Once the Certificate Manager is deployed, then we can establish ClusterIssuer operators in the cluster to support use of TLS with service Ingress endpoints. For Letsencrypt we can define two ClusterIssuer - for production and for staging . NOTE that these require the cluster to be publicly accessible, in order for the http01 acme flow to verify the domain ownership. Local development deployments will typically not have public IP/DNS - in which case the system deployment can proceed, but without TLS support for the service endpoints. Production \u2693\ufe0e apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-production spec: acme: # You must replace this email address with your own. # Let's Encrypt will use this to contact you about expiring # certificates, and issues related to your account. email: eoepca.systemteam@telespazio.com server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: # Secret resource that will be used to store the account's private key. name: letsencrypt-production-account-key # Add a single challenge solver, HTTP01 using nginx solvers: - http01: ingress: class: nginx Staging \u2693\ufe0e apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # You must replace this email address with your own. # Let's Encrypt will use this to contact you about expiring # certificates, and issues related to your account. email: eoepca.systemteam@telespazio.com server: https://acme-staging-v02.api.letsencrypt.org/directory privateKeySecretRef: # Secret resource that will be used to store the account's private key. name: letsencrypt-staging-account-key # Add a single challenge solver, HTTP01 using nginx solvers: - http01: ingress: class: nginx To exploit the specified ClusterIssuer the cert-manager.io/cluster-issuer annotation must be applied to the Ingress resource. For example\u2026 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx cert-manager.io/cluster-issuer: letsencrypt-production ... Sealed Secrets \u2693\ufe0e The EOEPCA development team maintain their deployment configurations in GitHub - for declarative, reproducible cluster deployments. Various Secret are relied upon by the system services. Secrets should not be exposed by commit to GitHub. Instead SealedSecret are committed to GitHub, which are encrypted, and can only be decrypted by the sealed-secret-controller that runs within the cluster. The sealed-secret-controller decrypts the SealedSecret to a regular Secret (of the same name) that can then be consumed by the cluster components. The sealed-secret-controller is deployed to the cluster using the helm chart\u2026 helm repo add bitnami-sealed-secrets https://bitnami-labs.github.io/sealed-secrets helm repo update helm install --version 1.13.2 --create-namespace --namespace infra \\ eoepca-sealed-secrets bitnami-sealed-secrets/sealed-secrets Once the controller is deployed within the cluster, then the kubeseal command can be used to create a SealedSecret from a regular Secret , as follows\u2026 Create example Secret\u2026 kubectl -n test create secret generic mysecret \\ --from-literal=password=changeme \\ --dry-run=client -o yaml \\ > mysecret.yaml Create SealedSecret from Secret using kubeseal\u2026 kubeseal -o yaml \\ --controller-name eoepca-sealed-secrets \\ --controller-namespace infra \\ < mysecret.yaml \\ > mysecret-sealed.yaml References \u2693\ufe0e Sealed Secrets on GitHub kubeseal Release MinIO Object Storage \u2693\ufe0e Various building blocks require access to an S3-compatible object storage service. In particular the ADES processing service expects to stage-out its processing results to S3 object storage. Ideally the cloud provider for your deployment will make available a suitable object storage service. As a workaround, in the absence of an existing object storage, it is possible to use MinIO to establish an object storage service within the Kubernetes cluster. We use the minio helm chart provided by bitnami . # Add the bitnami helm repository helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update # Install the minio helm chart helm upgrade -i minio -f minio-values.yaml bitnami/minio The minio deployment is customised via the values file minio-values.yaml , for example\u2026 auth: rootUser: eoepca rootPassword: changeme ingress: enabled: true ingressClassName: nginx hostname: minio-console.172.17.0.1.nip.io apiIngress: enabled: true ingressClassName: nginx hostname: minio.172.17.0.1.nip.io persistence: storageClass: local-path s3cmd Configuration \u2693\ufe0e The s3cmd can be configured for access to the MinIO deployment. The --configure option can be used to prepare a suitable configuration file for s3cmd \u2026 s3cmd -c mys3cfg --configure In response to the prompts, the following configuration selections are applicable to the above settings\u2026 Access Key: eoepca Secret Key: changeme Default Region: us-east-1 S3 Endpoint: minio.172.17.0.1.nip.io DNS-style bucket+hostname:port template for accessing a bucket: minio.172.17.0.1.nip.io Encryption password: Path to GPG program: /usr/bin/gpg Use HTTPS protocol: False HTTP Proxy server name: HTTP Proxy server port: 0 Save the configuration file, and check access to the S3 object store with\u2026 # Create a bucket s3cmd -c mys3cfg mb s3://eoepca # List buckets s3cmd -c mys3cfg ls References \u2693\ufe0e MinIO Helm Chart MinIO Helm Chart on GitHub","title":"Cluster Prerequisites"},{"location":"deployment-guide/cluster-prerequisites/#cluster-prerequisites","text":"The following prerequisite components are assumed to be deployed in the cluster.","title":"Cluster Prerequisites"},{"location":"deployment-guide/cluster-prerequisites/#nginx-ingress-controller","text":"# Add the helm repository helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update # Install the Nginx Ingress Controller helm chart helm upgrade -i ingress-nginx ingress-nginx/ingress-nginx --wait To target the Nginx Ingress Controller the kubernetes.io/ingress.class: nginx annotation must be applied to the Ingress resource\u2026 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx ...","title":"Nginx Ingress Controller"},{"location":"deployment-guide/cluster-prerequisites/#cert-manager","text":"# Add the helm repository helm repo add jetstack https://charts.jetstack.io helm repo update # Install the Cert Manager helm chart helm upgrade -i cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs=true","title":"Cert Manager"},{"location":"deployment-guide/cluster-prerequisites/#letsencrypt-certificates","text":"Once the Certificate Manager is deployed, then we can establish ClusterIssuer operators in the cluster to support use of TLS with service Ingress endpoints. For Letsencrypt we can define two ClusterIssuer - for production and for staging . NOTE that these require the cluster to be publicly accessible, in order for the http01 acme flow to verify the domain ownership. Local development deployments will typically not have public IP/DNS - in which case the system deployment can proceed, but without TLS support for the service endpoints.","title":"Letsencrypt Certificates"},{"location":"deployment-guide/cluster-prerequisites/#production","text":"apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-production spec: acme: # You must replace this email address with your own. # Let's Encrypt will use this to contact you about expiring # certificates, and issues related to your account. email: eoepca.systemteam@telespazio.com server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: # Secret resource that will be used to store the account's private key. name: letsencrypt-production-account-key # Add a single challenge solver, HTTP01 using nginx solvers: - http01: ingress: class: nginx","title":"Production"},{"location":"deployment-guide/cluster-prerequisites/#staging","text":"apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # You must replace this email address with your own. # Let's Encrypt will use this to contact you about expiring # certificates, and issues related to your account. email: eoepca.systemteam@telespazio.com server: https://acme-staging-v02.api.letsencrypt.org/directory privateKeySecretRef: # Secret resource that will be used to store the account's private key. name: letsencrypt-staging-account-key # Add a single challenge solver, HTTP01 using nginx solvers: - http01: ingress: class: nginx To exploit the specified ClusterIssuer the cert-manager.io/cluster-issuer annotation must be applied to the Ingress resource. For example\u2026 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx cert-manager.io/cluster-issuer: letsencrypt-production ...","title":"Staging"},{"location":"deployment-guide/cluster-prerequisites/#sealed-secrets","text":"The EOEPCA development team maintain their deployment configurations in GitHub - for declarative, reproducible cluster deployments. Various Secret are relied upon by the system services. Secrets should not be exposed by commit to GitHub. Instead SealedSecret are committed to GitHub, which are encrypted, and can only be decrypted by the sealed-secret-controller that runs within the cluster. The sealed-secret-controller decrypts the SealedSecret to a regular Secret (of the same name) that can then be consumed by the cluster components. The sealed-secret-controller is deployed to the cluster using the helm chart\u2026 helm repo add bitnami-sealed-secrets https://bitnami-labs.github.io/sealed-secrets helm repo update helm install --version 1.13.2 --create-namespace --namespace infra \\ eoepca-sealed-secrets bitnami-sealed-secrets/sealed-secrets Once the controller is deployed within the cluster, then the kubeseal command can be used to create a SealedSecret from a regular Secret , as follows\u2026 Create example Secret\u2026 kubectl -n test create secret generic mysecret \\ --from-literal=password=changeme \\ --dry-run=client -o yaml \\ > mysecret.yaml Create SealedSecret from Secret using kubeseal\u2026 kubeseal -o yaml \\ --controller-name eoepca-sealed-secrets \\ --controller-namespace infra \\ < mysecret.yaml \\ > mysecret-sealed.yaml","title":"Sealed Secrets"},{"location":"deployment-guide/cluster-prerequisites/#references","text":"Sealed Secrets on GitHub kubeseal Release","title":"References"},{"location":"deployment-guide/cluster-prerequisites/#minio-object-storage","text":"Various building blocks require access to an S3-compatible object storage service. In particular the ADES processing service expects to stage-out its processing results to S3 object storage. Ideally the cloud provider for your deployment will make available a suitable object storage service. As a workaround, in the absence of an existing object storage, it is possible to use MinIO to establish an object storage service within the Kubernetes cluster. We use the minio helm chart provided by bitnami . # Add the bitnami helm repository helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update # Install the minio helm chart helm upgrade -i minio -f minio-values.yaml bitnami/minio The minio deployment is customised via the values file minio-values.yaml , for example\u2026 auth: rootUser: eoepca rootPassword: changeme ingress: enabled: true ingressClassName: nginx hostname: minio-console.172.17.0.1.nip.io apiIngress: enabled: true ingressClassName: nginx hostname: minio.172.17.0.1.nip.io persistence: storageClass: local-path","title":"MinIO Object Storage"},{"location":"deployment-guide/cluster-prerequisites/#s3cmd-configuration","text":"The s3cmd can be configured for access to the MinIO deployment. The --configure option can be used to prepare a suitable configuration file for s3cmd \u2026 s3cmd -c mys3cfg --configure In response to the prompts, the following configuration selections are applicable to the above settings\u2026 Access Key: eoepca Secret Key: changeme Default Region: us-east-1 S3 Endpoint: minio.172.17.0.1.nip.io DNS-style bucket+hostname:port template for accessing a bucket: minio.172.17.0.1.nip.io Encryption password: Path to GPG program: /usr/bin/gpg Use HTTPS protocol: False HTTP Proxy server name: HTTP Proxy server port: 0 Save the configuration file, and check access to the S3 object store with\u2026 # Create a bucket s3cmd -c mys3cfg mb s3://eoepca # List buckets s3cmd -c mys3cfg ls","title":"s3cmd Configuration"},{"location":"deployment-guide/cluster-prerequisites/#references_1","text":"MinIO Helm Chart MinIO Helm Chart on GitHub","title":"References"},{"location":"deployment-guide/helm-repositories/","text":"Helm Repositories \u2693\ufe0e EOEPCA Helm Charts \u2693\ufe0e The EOEPCA building-blocks are engineered as containers for deployment to a Kubernetes cluster. Each building block defines a Helm Chart to facilitate its deployment. The EOEPCA Helm Chart Repository is configured with helm as follows\u2026 helm repo add eoepca https://eoepca.github.io/helm-charts/ Third-party Helm Charts \u2693\ufe0e In addition to the EOEPCA Helm Chart Repository, the following repositories are also relied upon, and should be configured\u2026 Cert Manager \u2693\ufe0e helm repo add jetstack https://charts.jetstack.io Nginx Ingress Controller \u2693\ufe0e helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Repo Update \u2693\ufe0e Refresh the local repo cache, after helm repo add \u2026 helm repo update","title":"Helm Repositories"},{"location":"deployment-guide/helm-repositories/#helm-repositories","text":"","title":"Helm Repositories"},{"location":"deployment-guide/helm-repositories/#eoepca-helm-charts","text":"The EOEPCA building-blocks are engineered as containers for deployment to a Kubernetes cluster. Each building block defines a Helm Chart to facilitate its deployment. The EOEPCA Helm Chart Repository is configured with helm as follows\u2026 helm repo add eoepca https://eoepca.github.io/helm-charts/","title":"EOEPCA Helm Charts"},{"location":"deployment-guide/helm-repositories/#third-party-helm-charts","text":"In addition to the EOEPCA Helm Chart Repository, the following repositories are also relied upon, and should be configured\u2026","title":"Third-party Helm Charts"},{"location":"deployment-guide/helm-repositories/#cert-manager","text":"helm repo add jetstack https://charts.jetstack.io","title":"Cert Manager"},{"location":"deployment-guide/helm-repositories/#nginx-ingress-controller","text":"helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx","title":"Nginx Ingress Controller"},{"location":"deployment-guide/helm-repositories/#repo-update","text":"Refresh the local repo cache, after helm repo add \u2026 helm repo update","title":"Repo Update"},{"location":"deployment-guide/kubernetes/","text":"Kubernetes Cluster \u2693\ufe0e The EOEPCA Reference Implementation has been developed with Kubernetes as its deployment target. The system components have been developed, deployed and tested using a cluster at version v1.18.10 . Rancher Kubernetes Engine (RKE) \u2693\ufe0e The development, integration and test clusters have been established using Rancher Kubernetes Engine (RKE) at version v1.18.10 . An example of the creation of the EOEPCA Kubernetes clusters can be found on the GitHub Kubernetes Setup page . CREODIAS has been used for the development hosting infrastructure - which provides OpenStack infrastructure that is backed by Cloudferro . An example of the Terraform configurations used to automate the creation of the cloud infrastructure that underpins the RKE deployment can be found on the GitHub CREODIAS Setup page . Local Kubernetes \u2693\ufe0e To make a full deployment of the EOEPCA Reference Implementation requires a multi-node node cluster with suitable resources. For example, the development cluster comprises: 1 Master node (2 vCPU, 8 GB RAM) 5 Worker nodes (4 vCPU, 16 GB RAM) 1 NFS server (2 vCPU, 8 GB RAM) Limited local deployment can be made using a suitable local single-node kuberbetes deployment using, for example, minikube or k3d . For example using k3d \u2026 k3d cluster create mycluster \\ -p \"80:80@loadbalancer\" \\ -p \"443:443@loadbalancer\" \\ --k3s-arg \"--disable=traefik@server:0\" \\ --agents 1 With such a deployment it is possible to deploy individual building-blocks for local development, or building-blocks in combination - within the constraints of the local host resources.","title":"Kubernetes Cluster"},{"location":"deployment-guide/kubernetes/#kubernetes-cluster","text":"The EOEPCA Reference Implementation has been developed with Kubernetes as its deployment target. The system components have been developed, deployed and tested using a cluster at version v1.18.10 .","title":"Kubernetes Cluster"},{"location":"deployment-guide/kubernetes/#rancher-kubernetes-engine-rke","text":"The development, integration and test clusters have been established using Rancher Kubernetes Engine (RKE) at version v1.18.10 . An example of the creation of the EOEPCA Kubernetes clusters can be found on the GitHub Kubernetes Setup page . CREODIAS has been used for the development hosting infrastructure - which provides OpenStack infrastructure that is backed by Cloudferro . An example of the Terraform configurations used to automate the creation of the cloud infrastructure that underpins the RKE deployment can be found on the GitHub CREODIAS Setup page .","title":"Rancher Kubernetes Engine (RKE)"},{"location":"deployment-guide/kubernetes/#local-kubernetes","text":"To make a full deployment of the EOEPCA Reference Implementation requires a multi-node node cluster with suitable resources. For example, the development cluster comprises: 1 Master node (2 vCPU, 8 GB RAM) 5 Worker nodes (4 vCPU, 16 GB RAM) 1 NFS server (2 vCPU, 8 GB RAM) Limited local deployment can be made using a suitable local single-node kuberbetes deployment using, for example, minikube or k3d . For example using k3d \u2026 k3d cluster create mycluster \\ -p \"80:80@loadbalancer\" \\ -p \"443:443@loadbalancer\" \\ --k3s-arg \"--disable=traefik@server:0\" \\ --agents 1 With such a deployment it is possible to deploy individual building-blocks for local development, or building-blocks in combination - within the constraints of the local host resources.","title":"Local Kubernetes"},{"location":"deployment-guide/login-service/","text":"Login Service \u2693\ufe0e The Login Service provides the platform Authorization Server for authenticated user identity and request authorization. Helm Chart \u2693\ufe0e The Login Service is deployed via the login-service helm chart from the EOEPCA Helm Chart Repository . The chart is configured via values that are fully documented in the README for the login-service chart . helm install --values login-service-values.yaml um-login-service eoepca/login-service Values \u2693\ufe0e At minimum, values for the following attributes should be specified: Public hostname of the Authorization Server, e.g. auth.172.17.0.1.nip.io IP Address of the public facing reverse proxy (Nginx Ingress Controller), e.g. 172.17.0.1 Name of Persistent Volume Claim for login-service persistence, e.g. eoepca-userman-pvc The boolen value volumeClaim.create can be used for the PVC to be created by the helm release. This creates a volume of type host-path and, hence, is only useful for single-node development usage. TLS Certificate Provider, e.g. letsencrypt-production Example login-service-values.yaml \u2026 volumeClaim: name: eoepca-userman-pvc create: false config: domain: auth.172.17.0.1.nip.io volumeClaim: name: eoepca-userman-pvc opendj: volumeClaim: name: eoepca-userman-pvc oxauth: volumeClaim: name: eoepca-userman-pvc oxtrust: volumeClaim: name: eoepca-userman-pvc global: domain: auth.172.17.0.1.nip.io nginxIp: 172.17.0.1 nginx: ingress: annotations: cert-manager.io/cluster-issuer: letsencrypt-production hosts: - auth.172.17.0.1.nip.io tls: - hosts: - auth.172.17.0.1.nip.io secretName: login-service-tls Additional Information \u2693\ufe0e Additional information regarding the Login Service can be found at: Helm Chart Wiki GitHub Repository","title":"Login Service"},{"location":"deployment-guide/login-service/#login-service","text":"The Login Service provides the platform Authorization Server for authenticated user identity and request authorization.","title":"Login Service"},{"location":"deployment-guide/login-service/#helm-chart","text":"The Login Service is deployed via the login-service helm chart from the EOEPCA Helm Chart Repository . The chart is configured via values that are fully documented in the README for the login-service chart . helm install --values login-service-values.yaml um-login-service eoepca/login-service","title":"Helm Chart"},{"location":"deployment-guide/login-service/#values","text":"At minimum, values for the following attributes should be specified: Public hostname of the Authorization Server, e.g. auth.172.17.0.1.nip.io IP Address of the public facing reverse proxy (Nginx Ingress Controller), e.g. 172.17.0.1 Name of Persistent Volume Claim for login-service persistence, e.g. eoepca-userman-pvc The boolen value volumeClaim.create can be used for the PVC to be created by the helm release. This creates a volume of type host-path and, hence, is only useful for single-node development usage. TLS Certificate Provider, e.g. letsencrypt-production Example login-service-values.yaml \u2026 volumeClaim: name: eoepca-userman-pvc create: false config: domain: auth.172.17.0.1.nip.io volumeClaim: name: eoepca-userman-pvc opendj: volumeClaim: name: eoepca-userman-pvc oxauth: volumeClaim: name: eoepca-userman-pvc oxtrust: volumeClaim: name: eoepca-userman-pvc global: domain: auth.172.17.0.1.nip.io nginxIp: 172.17.0.1 nginx: ingress: annotations: cert-manager.io/cluster-issuer: letsencrypt-production hosts: - auth.172.17.0.1.nip.io tls: - hosts: - auth.172.17.0.1.nip.io secretName: login-service-tls","title":"Values"},{"location":"deployment-guide/login-service/#additional-information","text":"Additional information regarding the Login Service can be found at: Helm Chart Wiki GitHub Repository","title":"Additional Information"},{"location":"deployment-guide/persistence/","text":"Persistence \u2693\ufe0e The EOEPCA building-blocks rely upon Kubernetes Persistent Volumes for their component persistence. Components integrate with the storage provided in the cluster by means of configurable Persistent Volume Claims and/or dynamic Storage Class that are specfied as values at time of deployment. Some components require storage of type ReadWriteMany - which, for a multi-node cluster, implies a network-based storage solution. ReadWriteMany Storage \u2693\ufe0e For the EOEPCA development deployment, an NFS server has been established to provide the persistence layer for ReadWriteMany storage. Pre-defined Persistent Volume Claims \u2693\ufe0e The EOEPCA development deployment establishes the following pre-defined Persistent Volume Claims, to provide a simple storage architecture that is organised around the \u2018domain areas\u2019 into which the Reference Implementation is split. Resource Managment ( resman ) Creates persistentvolume/eoepca-resman-pv and persistentvolumeclaim/eoepca-resman-pvc . Processing & Chaining ( proc ) Creates persistentvolume/eoepca-proc-pv and persistentvolumeclaim/eoepca-proc-pvc . User Management ( userman ) Creates persistentvolume/eoepca-userman-pv and persistentvolumeclaim/eoepca-userman-pvc . NOTE that this is offered only as an example thay suits the approach of the development team. Each building-block has configuration through which its persistence (PV/PVC) can be configured according the needs of the deployment. The \u2018domain-area-based\u2019 approach is acheived through the storage helm chart , with the following typical configuration\u2026 host: enabled: false nfs: enabled: true storageClass: eoepca-nfs server: address: \"<your-nfs-ip-address-here>\" domain: resman: enabled: true storageClass: eoepca-nfs proc: enabled: true storageClass: eoepca-nfs userman: enabled: true storageClass: eoepca-nfs Once established, these PV/PVCs are then referenced within the deployment configurations of the building-blocks. Dynamic ReadWriteMany Storage Provisioning \u2693\ufe0e In addition to the pre-defined PV/PVCs, the EOEPCA Reference Implementation also defines NFS-based storage classes for dynamic storage provisioning: managed-nfs-storage With a Reclaim Policy of Delete . managed-nfs-storage-retain With a Reclaim Policy of Retain . The building-blocks simply reference the required Storage Class in their volume specifications, to receive a Persistent Volume Claim that is dynamically provisioned at deployment time. This is acheived through the nfs-provisioner helm chart , with the following typical configurations\u2026 Reclaim Policy Delete \u2026 provisionerName: nfs-storage storageClass: name: managed-nfs-storage create: true reclaimPolicy: Delete archiveOnDelete: false allowVolumeExpansion: true nfs: server: \"<your-nfs-ip-address-here>\" path: /data/dynamic # your NFS server path here Reclaim Policy Retain \u2026 provisionerName: nfs-storage-retain storageClass: name: managed-nfs-storage-retain create: true reclaimPolicy: Retain allowVolumeExpansion: true nfs: server: \"<your-nfs-ip-address-here>\" path: /data/dynamic # your NFS server path here Clustered Storage Solutions \u2693\ufe0e Clustered storage approaches offer an alternative to NFS. Clustered Storage provides a network-attached storage through a set of commodity hosts whose storage is aggregated to form a distributed file-system. Capacity is scaled by adding additional nodes or adding additional storage to the existing nodes. In the context of a multi-node Kubernetes cluster, then it is typical that the same commodity nodes provide both the cluster members and storage resources, i.e. the clustered storage is spread across the Kubernetes worker nodes. Candidate clustered storage solutions include: GlusterFS GlusterFS is deployed as an operating system service across each node participating in the storage solution. Thus, with GlusterFS, the distributed storage nodes do not need to be one-and-the-same with the compute (cluster) nodes \u2013 although this may preferably be the case. Longhorn Longhorn offers a solution that is similar to that of GlusterFS, except that Longhorn is \u2018cloud-native\u2019 in that its service layer deploys within the Kubernetes cluster itself. Thus, the storage nodes are also the cluster compute nodes by design. All things being equal, Longhorn is recommended as the best approach for Kubernetes clusters. Local Cluster Storage \u2693\ufe0e For the purposes of the EOEPCA deployment, the default Storage Class included with the local Kubernetes distribution can be used for all storage concerns - e.g. host-only for minikube , local-path for k3d . The Rancher Local Path Provisioner ( local-path ) can be installed to any cluster in accordance the instructions on GitHub \u2026 kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml","title":"Persistence"},{"location":"deployment-guide/persistence/#persistence","text":"The EOEPCA building-blocks rely upon Kubernetes Persistent Volumes for their component persistence. Components integrate with the storage provided in the cluster by means of configurable Persistent Volume Claims and/or dynamic Storage Class that are specfied as values at time of deployment. Some components require storage of type ReadWriteMany - which, for a multi-node cluster, implies a network-based storage solution.","title":"Persistence"},{"location":"deployment-guide/persistence/#readwritemany-storage","text":"For the EOEPCA development deployment, an NFS server has been established to provide the persistence layer for ReadWriteMany storage.","title":"ReadWriteMany Storage"},{"location":"deployment-guide/persistence/#pre-defined-persistent-volume-claims","text":"The EOEPCA development deployment establishes the following pre-defined Persistent Volume Claims, to provide a simple storage architecture that is organised around the \u2018domain areas\u2019 into which the Reference Implementation is split. Resource Managment ( resman ) Creates persistentvolume/eoepca-resman-pv and persistentvolumeclaim/eoepca-resman-pvc . Processing & Chaining ( proc ) Creates persistentvolume/eoepca-proc-pv and persistentvolumeclaim/eoepca-proc-pvc . User Management ( userman ) Creates persistentvolume/eoepca-userman-pv and persistentvolumeclaim/eoepca-userman-pvc . NOTE that this is offered only as an example thay suits the approach of the development team. Each building-block has configuration through which its persistence (PV/PVC) can be configured according the needs of the deployment. The \u2018domain-area-based\u2019 approach is acheived through the storage helm chart , with the following typical configuration\u2026 host: enabled: false nfs: enabled: true storageClass: eoepca-nfs server: address: \"<your-nfs-ip-address-here>\" domain: resman: enabled: true storageClass: eoepca-nfs proc: enabled: true storageClass: eoepca-nfs userman: enabled: true storageClass: eoepca-nfs Once established, these PV/PVCs are then referenced within the deployment configurations of the building-blocks.","title":"Pre-defined Persistent Volume Claims"},{"location":"deployment-guide/persistence/#dynamic-readwritemany-storage-provisioning","text":"In addition to the pre-defined PV/PVCs, the EOEPCA Reference Implementation also defines NFS-based storage classes for dynamic storage provisioning: managed-nfs-storage With a Reclaim Policy of Delete . managed-nfs-storage-retain With a Reclaim Policy of Retain . The building-blocks simply reference the required Storage Class in their volume specifications, to receive a Persistent Volume Claim that is dynamically provisioned at deployment time. This is acheived through the nfs-provisioner helm chart , with the following typical configurations\u2026 Reclaim Policy Delete \u2026 provisionerName: nfs-storage storageClass: name: managed-nfs-storage create: true reclaimPolicy: Delete archiveOnDelete: false allowVolumeExpansion: true nfs: server: \"<your-nfs-ip-address-here>\" path: /data/dynamic # your NFS server path here Reclaim Policy Retain \u2026 provisionerName: nfs-storage-retain storageClass: name: managed-nfs-storage-retain create: true reclaimPolicy: Retain allowVolumeExpansion: true nfs: server: \"<your-nfs-ip-address-here>\" path: /data/dynamic # your NFS server path here","title":"Dynamic ReadWriteMany Storage Provisioning"},{"location":"deployment-guide/persistence/#clustered-storage-solutions","text":"Clustered storage approaches offer an alternative to NFS. Clustered Storage provides a network-attached storage through a set of commodity hosts whose storage is aggregated to form a distributed file-system. Capacity is scaled by adding additional nodes or adding additional storage to the existing nodes. In the context of a multi-node Kubernetes cluster, then it is typical that the same commodity nodes provide both the cluster members and storage resources, i.e. the clustered storage is spread across the Kubernetes worker nodes. Candidate clustered storage solutions include: GlusterFS GlusterFS is deployed as an operating system service across each node participating in the storage solution. Thus, with GlusterFS, the distributed storage nodes do not need to be one-and-the-same with the compute (cluster) nodes \u2013 although this may preferably be the case. Longhorn Longhorn offers a solution that is similar to that of GlusterFS, except that Longhorn is \u2018cloud-native\u2019 in that its service layer deploys within the Kubernetes cluster itself. Thus, the storage nodes are also the cluster compute nodes by design. All things being equal, Longhorn is recommended as the best approach for Kubernetes clusters.","title":"Clustered Storage Solutions"},{"location":"deployment-guide/persistence/#local-cluster-storage","text":"For the purposes of the EOEPCA deployment, the default Storage Class included with the local Kubernetes distribution can be used for all storage concerns - e.g. host-only for minikube , local-path for k3d . The Rancher Local Path Provisioner ( local-path ) can be installed to any cluster in accordance the instructions on GitHub \u2026 kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml","title":"Local Cluster Storage"},{"location":"deployment-guide/resource-protection/","text":"Resource Protection \u2693\ufe0e EOEPCA defines Building Blocks within a micro-service architecture. The services are subject to protection within an Identity and Access Management (IAM) approach that includes: Login Service (Authorization Server) Policy Decision Point (PDP) Policy Enforcement Point (PEP) Building Blocks that act as a Resource Server are individually protected by a Policy Enforcement Point (PEP). The PEP enforces the authorization decision in collaboration with the Login Service and Policy Decision Point (PDP). The PEP expects to interface to a client (user agent, e.g. browser) using User Managed Access (UMA) flows. It is not typical for a client to support UMA flows , and so the PEP can be deployed with a companion UMA User Agent component that interfaces between the client and the PEP, and performs the UMA Flow on behalf of the client. The Resource Guard is a \u2018convenience\u2019 component that deploys the PEP & UMA User Agent as a cooperating pair. The Resource Guard \u2018inserts itself\u2019 into the request path of the target Resource Server using the auth_request facility offered by Nginx. Thus, the Resource Guard deploys with an Ingress specification that: Configures the auth_request module to defer access authorization to the uma-user-agent service Configures the ingress rules (host/path) for the target Resource Server Helm Chart \u2693\ufe0e The Resource Guard is deployed via the resource-guard helm chart from the EOEPCA Helm Chart Repository . The chart is configured via values that are fully documented in the README for the resource-guard chart . It is expected to deploy multiple instances of the Resource Guard chart, one for each Resource Server to be protected. helm install --values myservice-guard-values.yaml myservice-guard eoepca/resource-guard Values \u2693\ufe0e The helm chart is deployed with values that are passed through to the subcharts for the pep-engine and uma-user-agent . Typical values to be specified include: Host/domain details for the Login Service and PDP, e.g. auth.172.17.0.1.nip.io IP Address of the public facing reverse proxy (Nginx Ingress Controller), e.g. 172.17.0.1 Name of Persistent Volume Claim for pep-engine persistence, e.g. myservice-pep-pvc TLS Certificate Provider, e.g. letsencrypt-production Optional specification of default resources with which to initialise the policy database for the component Ingress rules definition for reverse-proxy to the target Resource Server Name of Secret that contains the client credentials used by the uma-user-agent to interface with the Login Service. See section Client Secret below Example myservice-guard-values.yaml \u2026 #--------------------------------------------------------------------------- # Global values #--------------------------------------------------------------------------- global: context: myservice pep: myservice-pep domain: 172.17.0.1.nip.io nginxIp: 172.17.0.1 certManager: clusterIssuer: letsencrypt-production #--------------------------------------------------------------------------- # PEP values #--------------------------------------------------------------------------- pep-engine: configMap: workingMode: PARTIAL asHostname: auth pdpHostname: auth customDefaultResources: - name: \"Eric's space\" description: \"Protected Access for eric to his space in myservice\" resource_uri: \"/ericspace\" scopes: [] default_owner: \"d3688daa-385d-45b0-8e04-2062e3e2cd86\" nginxIntegration: enabled: false volumeClaim: name: myservice-pep-pvc create: false #--------------------------------------------------------------------------- # UMA User Agent values #--------------------------------------------------------------------------- uma-user-agent: fullnameOverride: myservice-agent nginxIntegration: enabled: true hosts: - host: myservice paths: - path: /(.*) service: name: myservice port: 80 - path: /(doc.*) service: name: myservice-docs port: 80 annotations: nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\" nginx.ingress.kubernetes.io/enable-cors: \"true\" nginx.ingress.kubernetes.io/rewrite-target: /$1 client: credentialsSecretName: \"myservice-agent\" logging: level: \"debug\" unauthorizedResponse: 'Bearer realm=\"https://auth.172.17.0.1.nip.io/oxauth/auth/passport/passportlogin.htm\"' #--------------------------------------------------------------------------- # END values #--------------------------------------------------------------------------- Client Credentials \u2693\ufe0e The uma-user-agent requires Client Credentials for its interactions with the login-service . The uma-user-agent expects to read these credentials from the file client.yaml , in the form\u2026 client-id: <my-client-id> client-secret: <my-secret> Client Registration \u2693\ufe0e To obtain the Client Credentials required by the uma-user-agent it is necessary to register a client with the login-service , or use the credentials for an existing client. A helper script is provided to register a basic client and obtain the required credentials. The script is available in the eoepca system repository , and can be obtained as follows\u2026 git clone git@github.com:EOEPCA/eoepca cd eoepca The register-client helper script requires some command-line arguments\u2026 Usage: register_client <authorization-server-hostname> <client-name> For example\u2026 ./bin/register-client auth.172.17.0.1.nip.io myclient INFO: Preparing docker image... [done] Client successfully registered. Make a note of the credentials: client_id = a98ba66e-e876-46e1-8619-5e130a38d1a4 client_secret = 73914cfc-c7dd-4b54-8807-ce17c3645558 NOTE that the register-client helper relies upon docker to build and run the script. Client Secret \u2693\ufe0e The client.yaml configuration file is made available via a Kubernetes Secret\u2026 kubectl -n myservice-ns create secret generic myservice-agent \\ --from-file=client.yaml \\ --dry-run=client -o yaml \\ > myservice-agent-secret.yaml apiVersion: v1 kind: Secret metadata: name: myservice-agent namespace: myservice-ns data: client.yaml: Y2xpZW50LWlkOiBhOThiYTY2ZS1lODc2LTQ2ZTEtODYxOS01ZTEzMGEzOGQxYTQKY2xpZW50LXNlY3JldDogNzM5MTRjZmMtYzdkZC00YjU0LTg4MDctY2UxN2MzNjQ1NTU4 The resource-guard deployment is configured with the name of the Secret through the helm chart value client.credentialsSecretName . Additional Information \u2693\ufe0e Additional information regarding the Resource Guard can be found at: Helm Chart README GitHub Repository: pep-engine uma-user-agent","title":"Resource Protection"},{"location":"deployment-guide/resource-protection/#resource-protection","text":"EOEPCA defines Building Blocks within a micro-service architecture. The services are subject to protection within an Identity and Access Management (IAM) approach that includes: Login Service (Authorization Server) Policy Decision Point (PDP) Policy Enforcement Point (PEP) Building Blocks that act as a Resource Server are individually protected by a Policy Enforcement Point (PEP). The PEP enforces the authorization decision in collaboration with the Login Service and Policy Decision Point (PDP). The PEP expects to interface to a client (user agent, e.g. browser) using User Managed Access (UMA) flows. It is not typical for a client to support UMA flows , and so the PEP can be deployed with a companion UMA User Agent component that interfaces between the client and the PEP, and performs the UMA Flow on behalf of the client. The Resource Guard is a \u2018convenience\u2019 component that deploys the PEP & UMA User Agent as a cooperating pair. The Resource Guard \u2018inserts itself\u2019 into the request path of the target Resource Server using the auth_request facility offered by Nginx. Thus, the Resource Guard deploys with an Ingress specification that: Configures the auth_request module to defer access authorization to the uma-user-agent service Configures the ingress rules (host/path) for the target Resource Server","title":"Resource Protection"},{"location":"deployment-guide/resource-protection/#helm-chart","text":"The Resource Guard is deployed via the resource-guard helm chart from the EOEPCA Helm Chart Repository . The chart is configured via values that are fully documented in the README for the resource-guard chart . It is expected to deploy multiple instances of the Resource Guard chart, one for each Resource Server to be protected. helm install --values myservice-guard-values.yaml myservice-guard eoepca/resource-guard","title":"Helm Chart"},{"location":"deployment-guide/resource-protection/#values","text":"The helm chart is deployed with values that are passed through to the subcharts for the pep-engine and uma-user-agent . Typical values to be specified include: Host/domain details for the Login Service and PDP, e.g. auth.172.17.0.1.nip.io IP Address of the public facing reverse proxy (Nginx Ingress Controller), e.g. 172.17.0.1 Name of Persistent Volume Claim for pep-engine persistence, e.g. myservice-pep-pvc TLS Certificate Provider, e.g. letsencrypt-production Optional specification of default resources with which to initialise the policy database for the component Ingress rules definition for reverse-proxy to the target Resource Server Name of Secret that contains the client credentials used by the uma-user-agent to interface with the Login Service. See section Client Secret below Example myservice-guard-values.yaml \u2026 #--------------------------------------------------------------------------- # Global values #--------------------------------------------------------------------------- global: context: myservice pep: myservice-pep domain: 172.17.0.1.nip.io nginxIp: 172.17.0.1 certManager: clusterIssuer: letsencrypt-production #--------------------------------------------------------------------------- # PEP values #--------------------------------------------------------------------------- pep-engine: configMap: workingMode: PARTIAL asHostname: auth pdpHostname: auth customDefaultResources: - name: \"Eric's space\" description: \"Protected Access for eric to his space in myservice\" resource_uri: \"/ericspace\" scopes: [] default_owner: \"d3688daa-385d-45b0-8e04-2062e3e2cd86\" nginxIntegration: enabled: false volumeClaim: name: myservice-pep-pvc create: false #--------------------------------------------------------------------------- # UMA User Agent values #--------------------------------------------------------------------------- uma-user-agent: fullnameOverride: myservice-agent nginxIntegration: enabled: true hosts: - host: myservice paths: - path: /(.*) service: name: myservice port: 80 - path: /(doc.*) service: name: myservice-docs port: 80 annotations: nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\" nginx.ingress.kubernetes.io/enable-cors: \"true\" nginx.ingress.kubernetes.io/rewrite-target: /$1 client: credentialsSecretName: \"myservice-agent\" logging: level: \"debug\" unauthorizedResponse: 'Bearer realm=\"https://auth.172.17.0.1.nip.io/oxauth/auth/passport/passportlogin.htm\"' #--------------------------------------------------------------------------- # END values #---------------------------------------------------------------------------","title":"Values"},{"location":"deployment-guide/resource-protection/#client-credentials","text":"The uma-user-agent requires Client Credentials for its interactions with the login-service . The uma-user-agent expects to read these credentials from the file client.yaml , in the form\u2026 client-id: <my-client-id> client-secret: <my-secret>","title":"Client Credentials"},{"location":"deployment-guide/resource-protection/#client-registration","text":"To obtain the Client Credentials required by the uma-user-agent it is necessary to register a client with the login-service , or use the credentials for an existing client. A helper script is provided to register a basic client and obtain the required credentials. The script is available in the eoepca system repository , and can be obtained as follows\u2026 git clone git@github.com:EOEPCA/eoepca cd eoepca The register-client helper script requires some command-line arguments\u2026 Usage: register_client <authorization-server-hostname> <client-name> For example\u2026 ./bin/register-client auth.172.17.0.1.nip.io myclient INFO: Preparing docker image... [done] Client successfully registered. Make a note of the credentials: client_id = a98ba66e-e876-46e1-8619-5e130a38d1a4 client_secret = 73914cfc-c7dd-4b54-8807-ce17c3645558 NOTE that the register-client helper relies upon docker to build and run the script.","title":"Client Registration"},{"location":"deployment-guide/resource-protection/#client-secret","text":"The client.yaml configuration file is made available via a Kubernetes Secret\u2026 kubectl -n myservice-ns create secret generic myservice-agent \\ --from-file=client.yaml \\ --dry-run=client -o yaml \\ > myservice-agent-secret.yaml apiVersion: v1 kind: Secret metadata: name: myservice-agent namespace: myservice-ns data: client.yaml: Y2xpZW50LWlkOiBhOThiYTY2ZS1lODc2LTQ2ZTEtODYxOS01ZTEzMGEzOGQxYTQKY2xpZW50LXNlY3JldDogNzM5MTRjZmMtYzdkZC00YjU0LTg4MDctY2UxN2MzNjQ1NTU4 The resource-guard deployment is configured with the name of the Secret through the helm chart value client.credentialsSecretName .","title":"Client Secret"},{"location":"deployment-guide/resource-protection/#additional-information","text":"Additional information regarding the Resource Guard can be found at: Helm Chart README GitHub Repository: pep-engine uma-user-agent","title":"Additional Information"}]}