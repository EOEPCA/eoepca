{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EO Exploitation Platform Common Architecture \u2693\ufe0e Deployment Guide","title":"Home"},{"location":"#eo-exploitation-platform-common-architecture","text":"Deployment Guide","title":"EO Exploitation Platform Common Architecture"},{"location":"about/","text":"About EOEPCA \u2693\ufe0e TBD","title":"About"},{"location":"about/#about-eoepca","text":"TBD","title":"About EOEPCA"},{"location":"deployment-guide/","text":"Deployment Guide \u2693\ufe0e The Deployment Guide describes how each building-block comprising the EOEPCA Reference Implementation is configured and deployed. A full system deployment is described, in which components are deployed with complementary configurations that facilitate their integration as a coherent system. Nevertheless, each component can be cherry-picked from this system deployment for individual re-use. The deployment is organised into the following sections: Kubernetes cluster Storage Login Service etc.","title":"Introduction"},{"location":"deployment-guide/#deployment-guide","text":"The Deployment Guide describes how each building-block comprising the EOEPCA Reference Implementation is configured and deployed. A full system deployment is described, in which components are deployed with complementary configurations that facilitate their integration as a coherent system. Nevertheless, each component can be cherry-picked from this system deployment for individual re-use. The deployment is organised into the following sections: Kubernetes cluster Storage Login Service etc.","title":"Deployment Guide"},{"location":"deployment-guide/cluster-prerequisites/","text":"Cluster Prerequisites \u2693\ufe0e The following prerequisite components are assumed to be deployed in the cluster. Nginx Ingress Controller \u2693\ufe0e # Add the helm repository helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update # Install the Nginx Ingress Controller helm chart helm upgrade -i ingress-nginx ingress-nginx/ingress-nginx --wait Cert Manager \u2693\ufe0e # Add the helm repository helm repo add jetstack https://charts.jetstack.io helm repo update # Install the Cert Manager helm chart helm upgrade -i cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs=true","title":"Cluster Prerequisites"},{"location":"deployment-guide/cluster-prerequisites/#cluster-prerequisites","text":"The following prerequisite components are assumed to be deployed in the cluster.","title":"Cluster Prerequisites"},{"location":"deployment-guide/cluster-prerequisites/#nginx-ingress-controller","text":"# Add the helm repository helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update # Install the Nginx Ingress Controller helm chart helm upgrade -i ingress-nginx ingress-nginx/ingress-nginx --wait","title":"Nginx Ingress Controller"},{"location":"deployment-guide/cluster-prerequisites/#cert-manager","text":"# Add the helm repository helm repo add jetstack https://charts.jetstack.io helm repo update # Install the Cert Manager helm chart helm upgrade -i cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs=true","title":"Cert Manager"},{"location":"deployment-guide/helm-repositories/","text":"Helm Repositories \u2693\ufe0e EOEPCA Helm Charts \u2693\ufe0e The EOEPCA building-blocks are engineered as containers for deployment to a Kubernetes cluster. Each building block defines a Helm Chart to facilitate its deployment. The EOEPCA Helm Chart Repository is configured with helm as follows\u2026 helm repo add eoepca https://eoepca.github.io/helm-charts/ Third-party Helm Charts \u2693\ufe0e In addition to the EOEPCA Helm Chart Repository, the following repositories are also relied upon, and should be configured\u2026 Cert Manager \u2693\ufe0e helm repo add jetstack https://charts.jetstack.io Nginx Ingress Controller \u2693\ufe0e helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Repo Update \u2693\ufe0e Refresh the local repo cache, after helm repo add \u2026 helm repo update","title":"Helm Repositories"},{"location":"deployment-guide/helm-repositories/#helm-repositories","text":"","title":"Helm Repositories"},{"location":"deployment-guide/helm-repositories/#eoepca-helm-charts","text":"The EOEPCA building-blocks are engineered as containers for deployment to a Kubernetes cluster. Each building block defines a Helm Chart to facilitate its deployment. The EOEPCA Helm Chart Repository is configured with helm as follows\u2026 helm repo add eoepca https://eoepca.github.io/helm-charts/","title":"EOEPCA Helm Charts"},{"location":"deployment-guide/helm-repositories/#third-party-helm-charts","text":"In addition to the EOEPCA Helm Chart Repository, the following repositories are also relied upon, and should be configured\u2026","title":"Third-party Helm Charts"},{"location":"deployment-guide/helm-repositories/#cert-manager","text":"helm repo add jetstack https://charts.jetstack.io","title":"Cert Manager"},{"location":"deployment-guide/helm-repositories/#nginx-ingress-controller","text":"helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx","title":"Nginx Ingress Controller"},{"location":"deployment-guide/helm-repositories/#repo-update","text":"Refresh the local repo cache, after helm repo add \u2026 helm repo update","title":"Repo Update"},{"location":"deployment-guide/kubernetes/","text":"Kubernetes Cluster \u2693\ufe0e The EOEPCA Reference Implementation has been developed with Kubernetes as its deployment target. The system components have been developed, deployed and tested using a cluster at version v1.18.10 . Rancher Kubernetes Engine (RKE) \u2693\ufe0e The development, integration and test clusters have been established using Rancher Kubernetes Engine (RKE) at version v1.18.10 . An example of the creation of the EOEPCA Kubernetes clusters can be found on the GitHub Kubernetes Setup page . CREODIAS has been used for the development hosting infrastructure - which provides OpenStack infrastructure that is backed by Cloudferro . An example of the Terraform configurations used to automate the creation of the cloud infrastructure that underpins the RKE deployment can be found on the GitHub CREODIAS Setup page . Local Kubernetes \u2693\ufe0e To make a full deployment of the EOEPCA Reference Implementation requires a multi-node node cluster with suitable resources. For example, the development cluster comprises: 1 Master node (2 vCPU, 8 GB RAM) 5 Worker nodes (4 vCPU, 16 GB RAM) 1 NFS server (2 vCPU, 8 GB RAM) Limited local deployment can be made using a suitable local single-node kuberbetes deployment using, for example, minikube or k3d . For example using k3d \u2026 k3d cluster create mycluster \\ -p \"80:80@loadbalancer\" \\ -p \"443:443@loadbalancer\" \\ --k3s-arg \"--disable=traefik@server:0\" \\ --agents 1 With such a deployment it is possible to deploy individual building-blocks for local development, or building-blocks in combination - within the constraints of the local host resources.","title":"Kubernetes Cluster"},{"location":"deployment-guide/kubernetes/#kubernetes-cluster","text":"The EOEPCA Reference Implementation has been developed with Kubernetes as its deployment target. The system components have been developed, deployed and tested using a cluster at version v1.18.10 .","title":"Kubernetes Cluster"},{"location":"deployment-guide/kubernetes/#rancher-kubernetes-engine-rke","text":"The development, integration and test clusters have been established using Rancher Kubernetes Engine (RKE) at version v1.18.10 . An example of the creation of the EOEPCA Kubernetes clusters can be found on the GitHub Kubernetes Setup page . CREODIAS has been used for the development hosting infrastructure - which provides OpenStack infrastructure that is backed by Cloudferro . An example of the Terraform configurations used to automate the creation of the cloud infrastructure that underpins the RKE deployment can be found on the GitHub CREODIAS Setup page .","title":"Rancher Kubernetes Engine (RKE)"},{"location":"deployment-guide/kubernetes/#local-kubernetes","text":"To make a full deployment of the EOEPCA Reference Implementation requires a multi-node node cluster with suitable resources. For example, the development cluster comprises: 1 Master node (2 vCPU, 8 GB RAM) 5 Worker nodes (4 vCPU, 16 GB RAM) 1 NFS server (2 vCPU, 8 GB RAM) Limited local deployment can be made using a suitable local single-node kuberbetes deployment using, for example, minikube or k3d . For example using k3d \u2026 k3d cluster create mycluster \\ -p \"80:80@loadbalancer\" \\ -p \"443:443@loadbalancer\" \\ --k3s-arg \"--disable=traefik@server:0\" \\ --agents 1 With such a deployment it is possible to deploy individual building-blocks for local development, or building-blocks in combination - within the constraints of the local host resources.","title":"Local Kubernetes"},{"location":"deployment-guide/storage/","text":"Cluster Storage \u2693\ufe0e The EOEPCA building-blocks rely upon Kubernetes Persistent Volumes for their component persistence. Components integrate with the storage provided in the cluster by means of configurable Persistent Volume Claims and/or dynamic Storage Class that are specfied as values at time of deployment. Some components require storage of type ReadWriteMany - which, for a multi-node cluster, implies a network-based storage solution. ReadWriteMany Storage \u2693\ufe0e For the EOEPCA development deployment, an NFS server has been established to provide the persistence layer for ReadWriteMany storage. Pre-defined Persistent Volume Claims \u2693\ufe0e The EOEPCA development deployment establishes the following pre-defined Persistent Volume Claims, to provide a simple storage architecture that is organised around the \u2018domain areas\u2019 into which the Reference Implementation is split: Resource Managment ( resman ) Creates persistentvolume/eoepca-resman-pv and persistentvolumeclaim/eoepca-resman-pvc . Processing & Chaining ( proc ) Creates persistentvolume/eoepca-proc-pv and persistentvolumeclaim/eoepca-proc-pvc . User Management ( userman ) Creates persistentvolume/eoepca-userman-pv and persistentvolumeclaim/eoepca-userman-pvc . This is acheived through the storage helm chart , with the following typical configuration\u2026 host: enabled: false nfs: enabled: true storageClass: eoepca-nfs server: address: \"<your-nfs-ip-address-here>\" domain: resman: enabled: true storageClass: eoepca-nfs proc: enabled: true storageClass: eoepca-nfs userman: enabled: true storageClass: eoepca-nfs Once established, these PV/PVCs are then referenced within the deployment configurations of the building-blocks. Dynamic ReadWriteMany Storage Provisioning \u2693\ufe0e In addition to the pre-defined PV/PVCs, the EOEPCA Reference Implementation also defines NFS-based storage classes for dynamic storage provisioning: managed-nfs-storage With a Reclaim Policy of Delete . managed-nfs-storage-retain With a Reclaim Policy of Retain . The building-blocks simply reference the required Storage Class in their volume specifications, to receive a Persistent Volume Claim that is dynamically provisioned at deployment time. This is acheived through the nfs-provisioner helm chart , with the following typical configurations\u2026 Reclaim Policy Delete \u2026 provisionerName: nfs-storage storageClass: name: managed-nfs-storage create: true reclaimPolicy: Delete archiveOnDelete: false allowVolumeExpansion: true nfs: server: \"<your-nfs-ip-address-here>\" path: /data/dynamic # your NFS server path here Reclaim Policy Retain \u2026 provisionerName: nfs-storage-retain storageClass: name: managed-nfs-storage-retain create: true reclaimPolicy: Retain allowVolumeExpansion: true nfs: server: \"<your-nfs-ip-address-here>\" path: /data/dynamic # your NFS server path here Clustered Storage Solutions \u2693\ufe0e Clustered storage approaches offer an alternative to NFS. Clustered Storage provides a network-attached storage through a set of commodity hosts whose storage is aggregated to form a distributed file-system. Capacity is scaled by adding additional nodes or adding additional storage to the existing nodes. In the context of a multi-node Kubernetes cluster, then it is typical that the same commodity nodes provide both the cluster members and storage resources, i.e. the clustered storage is spread across the Kubernetes worker nodes. Candidate clustered storage solutions include: GlusterFS GlusterFS is deployed as an operating system service across each node participating in the storage solution. Thus, with GlusterFS, the distributed storage nodes do not need to be one-and-the-same with the compute (cluster) nodes \u2013 although this may preferably be the case. Longhorn Longhorn offers a solution that is similar to that of GlusterFS, except that Longhorn is \u2018cloud-native\u2019 in that its service layer deploys within the Kubernetes cluster itself. Thus, the storage nodes are also the cluster compute nodes by design. All things being equal, Longhorn is recommended as the best approach for Kubernetes clusters. Local Cluster Storage \u2693\ufe0e For local development Kubernetes clusters then ReadWriteMany ceases to be a consideration. Thus, for the purposes of the EOEPCA deployment, the default Storage Class included with the local Kubernetes distribution can be used for all storage concerns - e.g. host-only for minikube , local-path for k3d . The Rancher Local Path Provisioner ( local-path ) can be installed to any cluster in accordance the instructions on GitHub \u2026 kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml","title":"Cluster Storage"},{"location":"deployment-guide/storage/#cluster-storage","text":"The EOEPCA building-blocks rely upon Kubernetes Persistent Volumes for their component persistence. Components integrate with the storage provided in the cluster by means of configurable Persistent Volume Claims and/or dynamic Storage Class that are specfied as values at time of deployment. Some components require storage of type ReadWriteMany - which, for a multi-node cluster, implies a network-based storage solution.","title":"Cluster Storage"},{"location":"deployment-guide/storage/#readwritemany-storage","text":"For the EOEPCA development deployment, an NFS server has been established to provide the persistence layer for ReadWriteMany storage.","title":"ReadWriteMany Storage"},{"location":"deployment-guide/storage/#pre-defined-persistent-volume-claims","text":"The EOEPCA development deployment establishes the following pre-defined Persistent Volume Claims, to provide a simple storage architecture that is organised around the \u2018domain areas\u2019 into which the Reference Implementation is split: Resource Managment ( resman ) Creates persistentvolume/eoepca-resman-pv and persistentvolumeclaim/eoepca-resman-pvc . Processing & Chaining ( proc ) Creates persistentvolume/eoepca-proc-pv and persistentvolumeclaim/eoepca-proc-pvc . User Management ( userman ) Creates persistentvolume/eoepca-userman-pv and persistentvolumeclaim/eoepca-userman-pvc . This is acheived through the storage helm chart , with the following typical configuration\u2026 host: enabled: false nfs: enabled: true storageClass: eoepca-nfs server: address: \"<your-nfs-ip-address-here>\" domain: resman: enabled: true storageClass: eoepca-nfs proc: enabled: true storageClass: eoepca-nfs userman: enabled: true storageClass: eoepca-nfs Once established, these PV/PVCs are then referenced within the deployment configurations of the building-blocks.","title":"Pre-defined Persistent Volume Claims"},{"location":"deployment-guide/storage/#dynamic-readwritemany-storage-provisioning","text":"In addition to the pre-defined PV/PVCs, the EOEPCA Reference Implementation also defines NFS-based storage classes for dynamic storage provisioning: managed-nfs-storage With a Reclaim Policy of Delete . managed-nfs-storage-retain With a Reclaim Policy of Retain . The building-blocks simply reference the required Storage Class in their volume specifications, to receive a Persistent Volume Claim that is dynamically provisioned at deployment time. This is acheived through the nfs-provisioner helm chart , with the following typical configurations\u2026 Reclaim Policy Delete \u2026 provisionerName: nfs-storage storageClass: name: managed-nfs-storage create: true reclaimPolicy: Delete archiveOnDelete: false allowVolumeExpansion: true nfs: server: \"<your-nfs-ip-address-here>\" path: /data/dynamic # your NFS server path here Reclaim Policy Retain \u2026 provisionerName: nfs-storage-retain storageClass: name: managed-nfs-storage-retain create: true reclaimPolicy: Retain allowVolumeExpansion: true nfs: server: \"<your-nfs-ip-address-here>\" path: /data/dynamic # your NFS server path here","title":"Dynamic ReadWriteMany Storage Provisioning"},{"location":"deployment-guide/storage/#clustered-storage-solutions","text":"Clustered storage approaches offer an alternative to NFS. Clustered Storage provides a network-attached storage through a set of commodity hosts whose storage is aggregated to form a distributed file-system. Capacity is scaled by adding additional nodes or adding additional storage to the existing nodes. In the context of a multi-node Kubernetes cluster, then it is typical that the same commodity nodes provide both the cluster members and storage resources, i.e. the clustered storage is spread across the Kubernetes worker nodes. Candidate clustered storage solutions include: GlusterFS GlusterFS is deployed as an operating system service across each node participating in the storage solution. Thus, with GlusterFS, the distributed storage nodes do not need to be one-and-the-same with the compute (cluster) nodes \u2013 although this may preferably be the case. Longhorn Longhorn offers a solution that is similar to that of GlusterFS, except that Longhorn is \u2018cloud-native\u2019 in that its service layer deploys within the Kubernetes cluster itself. Thus, the storage nodes are also the cluster compute nodes by design. All things being equal, Longhorn is recommended as the best approach for Kubernetes clusters.","title":"Clustered Storage Solutions"},{"location":"deployment-guide/storage/#local-cluster-storage","text":"For local development Kubernetes clusters then ReadWriteMany ceases to be a consideration. Thus, for the purposes of the EOEPCA deployment, the default Storage Class included with the local Kubernetes distribution can be used for all storage concerns - e.g. host-only for minikube , local-path for k3d . The Rancher Local Path Provisioner ( local-path ) can be installed to any cluster in accordance the instructions on GitHub \u2026 kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml","title":"Local Cluster Storage"}]}