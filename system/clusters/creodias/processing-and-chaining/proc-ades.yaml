apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: proc-ades
  namespace: proc
spec:
  chart:
    spec:
      chart: ades
      version: 2.0.11
      sourceRef:
        kind: HelmRepository
        name: eoepca
        namespace: common
  values:
    # values - start
    replicaCount: 1
    image:
      pullPolicy: Always
      # Overrides the image tag whose default is the chart appVersion.
      # tag: "2.0.5"
    nameOverride: ""
    fullnameOverride: ""
    serviceAccount:
      # Specifies whether a service account should be created
      create: true
      # Annotations to add to the service account
      annotations: {}
      # The name of the service account to use.
      # If not set and create is true, a name is generated using the fullname template
      name: ""
    podAnnotations: {}
    podSecurityContext:
      {}
      # fsGroup: 2000
    securityContext:
      {}
      # capabilities:
      #   drop:
      #   - ALL
      # readOnlyRootFilesystem: true
      # runAsNonRoot: true
      # runAsUser: 1000
    service:
      type: ClusterIP
      port: 80
    ingress:
      enabled: true
      annotations: {}
      hosts:
        - host: ades-open.develop.eoepca.org
          paths:
            - path: /
              pathType: ImplementationSpecific    
      tls: []
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local
    resources:
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      limits:
        cpu: 2
        memory: 4Gi
      requests:
        cpu: 500m
        memory: 2Gi
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 3
      targetCPUUtilizationPercentage: 80
      # targetMemoryUtilizationPercentage: 80
    storageClassName: managed-nfs-storage
    clusterAdminRoleName: cluster-admin
    nodeSelector: {}
    tolerations: []
    affinity: {}
    useKubeProxy: True
    workflowExecutor:
      # Necessary if useKubeProxy set to false
      # kubeconfig: "files/kubeconfig"

      # Here specify fixed inputs to all workflows execution in all stages (main, stage-in/out)
      # They will be prefixed with 'ADES_'. e.g. 'APP: ades' will be 'ADES_APP: ades'
      inputs:
        APP: ades

        STAGEIN_AWS_SERVICEURL: http://data.cloudferro.com
        STAGEIN_AWS_ACCESS_KEY_ID: test
        STAGEIN_AWS_SECRET_ACCESS_KEY: test
        STAGEIN_AWS_REGION: RegioneOne

        STAGEOUT_AWS_SERVICEURL: https://cf2.cloudferro.com:8080
      #  STAGEOUT_AWS_ACCESS_KEY_ID: 4ceda7292edc4cc997d133079b469900
      #  STAGEOUT_AWS_SECRET_ACCESS_KEY: 7d824cbfc90147318fa6ac7fe0e9a174
      #  STAGEOUT_AWS_REGION: RegionOne
      #  STAGEOUT_OUTPUT: s3://processing_results

      #    STAGEOUT_STORAGE_HOST: https://nx13206.your-storageshare.de/
      #    STAGEOUT_STORAGE_USERNAME: eoepca-storagecd 
      #    STAGEOUT_STORAGE_APIKEY: 4k8wMajA5ABaYdk

      useResourceManager: "true"
      resourceManagerWorkspacePrefix: "develop-user"
      resourceManagerEndpoint: "https://workspace-api.develop.eoepca.org"
      platformDomain: "https://auth.develop.eoepca.org"

      pod_env_vars:
        yaml: "{}"


      stageout:
        cwl: |
          cwlVersion: v1.0
          doc: "Run Stars for staging results"
          class: CommandLineTool
          hints:
            DockerRequirement:
              dockerPull: terradue/stars:2.3.0
            "cwltool:Secrets":
              secrets:
                - ADES_STAGEOUT_AWS_SERVICEURL
                - ADES_STAGEOUT_AWS_REGION
                - ADES_STAGEOUT_AWS_ACCESS_KEY_ID
                - ADES_STAGEOUT_AWS_SECRET_ACCESS_KEY
          id: stars
          inputs:
            ADES_STAGEOUT_AWS_SERVICEURL:
              type: string?
            ADES_STAGEOUT_AWS_ACCESS_KEY_ID:
              type: string?
            ADES_STAGEOUT_AWS_SECRET_ACCESS_KEY:
              type: string?
            ADES_STAGEOUT_OUTPUT:
              type: string?
            ADES_STAGEOUT_AWS_REGION:
              type: string?
            process:
              type: string
          outputs:
            s3_catalog_output:
              outputBinding:
                outputEval: ${ return inputs.ADES_STAGEOUT_OUTPUT + "/" + inputs.process + "/catalog.json"; }
              type: string
          baseCommand: ['/bin/bash', 'stageout.sh']
          requirements:
            InlineJavascriptRequirement: {}
            EnvVarRequirement:
              envDef:
                PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
            ResourceRequirement: {}
            InitialWorkDirRequirement:
              listing:
              - entryname: stageout.sh
                entry: |-
                  #!/bin/bash
                  AWS__ServiceURL=$(inputs.ADES_STAGEOUT_AWS_SERVICEURL)
                  AWS__Region=$(inputs.ADES_STAGEOUT_AWS_REGION)
                  AWS__AuthenticationRegion=$(inputs.ADES_STAGEOUT_AWS_REGION)
                  AWS_ACCESS_KEY_ID=$(inputs.ADES_STAGEOUT_AWS_ACCESS_KEY_ID)
                  AWS_SECRET_ACCESS_KEY=$(inputs.ADES_STAGEOUT_AWS_SECRET_ACCESS_KEY)

                  Stars copy -v -r 4 -o $( inputs.ADES_STAGEOUT_OUTPUT + "/" + inputs.process ) \
                  $(inputs.wf_outputs.path) + "/catalog.json"


      stagein:
        cwl: |
          cwlVersion: v1.0
          doc: "Run Stars for staging input data"
          class: CommandLineTool
          hints:
            DockerRequirement:
              dockerPull: terradue/stars:1.0.0-beta.11
            "cwltool:Secrets":
              secrets:
              - ADES_STAGEIN_AWS_SERVICEURL
              - ADES_STAGEIN_AWS_ACCESS_KEY_ID
              - ADES_STAGEIN_AWS_SECRET_ACCESS_KEY
          id: stars
          inputs:
            ADES_STAGEIN_AWS_SERVICEURL:
              type: string?
            ADES_STAGEIN_AWS_ACCESS_KEY_ID:
              type: string?
            ADES_STAGEIN_AWS_SECRET_ACCESS_KEY:
              type: string?
          outputs: {}
          baseCommand: ['/bin/bash', 'stagein.sh']
          requirements:

            InitialWorkDirRequirement:
              listing:
              - entryname: stagein.sh
                entry: |-
                  #!/bin/bash
                  export AWS__ServiceURL=$(inputs.ADES_STAGEIN_AWS_SERVICEURL)
                  export AWS_ACCESS_KEY_ID=$(inputs.ADES_STAGEIN_AWS_ACCESS_KEY_ID)
                  export AWS_SECRET_ACCESS_KEY=$(inputs.ADES_STAGEIN_AWS_SECRET_ACCESS_KEY)
                  Stars copy -v -rel -r 4 -o ./ --harvest $1
            EnvVarRequirement:
              envDef:
                PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
            ResourceRequirement: {}



      # kubernetes storage class to be used for provisioning volumes. Must be a persistent volume claim compliant (glusterfs-storage)
      processingStorageClass: managed-nfs-storage
      # Size of the Kubernetes Tmp Volumes
      processingVolumeTmpSize: "6Gi"
      # Size of the Kubernetes Output Volumes
      processingVolumeOutputSize: "6Gi"
      # Max ram to use for a job
      processingMaxRam: "8Gi"
      # Max number of CPU cores to use concurrently for a job
      processingMaxCores: "4"
      # if false the Ades will clean the volume after the workflow has successfully finished running
      processingKeepWorkspace: "True"
      processingKeepWorkspaceIfFailed: "True"
      # image pull secrects
      imagePullSecrets: []
      #  custom stagein cwl
      #stageincwl: "files/stageincwl.cwl"
      #maincwl: "files/maincwlmetrics.cwl"

      pod:
        env: {}

      # custom backoff limit for calrissian job
      backofflimit: 3
      
        
    wps:
      pepBaseUrl: "http://ades-pep:5576"
      usePep: "true"
      maincfgtpl: "files/main.cfg.tpl"
    persistence:
      enabled: true
      # existingUserDataClaim:
      # existingProcServicesClaim:
      storageClass: "managed-nfs-storage"
      userDataAccessMode: ReadWriteOnce
      userDataSize: 10Gi
      procServicesAccessMode: ReadWriteOnce
      procServicesSize: 5Gi
    # values - end
  # timeout: 25m0s
  interval: 1m0s
